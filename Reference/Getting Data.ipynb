{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "getting_data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mxzWG0juqnC"
      },
      "source": [
        "# **Chapter 9. Getting Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWHNFLR8uzeD"
      },
      "source": [
        "## Reading Files\n",
        "\n",
        "* The easiest way of handing a CSV file is to use Pandas (not covered in our textbook).\n",
        "![picture](https://drive.google.com/uc?id=1REB_9aobuG1ZeLtXkT6gFWII2Rz8CWBX)\n",
        "* In addition to the CSV file, Pandas provides the funcions for loading the input files of various formats (e.g., Excel)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs1pKWXq-tcu"
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "from six.moves import urllib\n",
        "\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
        "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
        "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
        "\n",
        "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
        "    if not os.path.isdir(housing_path):\n",
        "        os.makedirs(housing_path)\n",
        "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
        "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
        "    housing_tgz = tarfile.open(tgz_path)\n",
        "    housing_tgz.extractall(path=housing_path)\n",
        "    housing_tgz.close()\n",
        "\n",
        "fetch_housing_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "pVah8whF-vfQ",
        "outputId": "1fd8323e-4262-4bfc-bed0-6bf5b5d18e9b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_housing_data(housing_path=HOUSING_PATH):\n",
        "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
        "    return pd.read_csv(csv_path)\n",
        "\n",
        "housing = load_housing_data()\n",
        "\n",
        "housing.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "      <th>ocean_proximity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-122.23</td>\n",
              "      <td>37.88</td>\n",
              "      <td>41.0</td>\n",
              "      <td>880.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>322.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>8.3252</td>\n",
              "      <td>452600.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-122.22</td>\n",
              "      <td>37.86</td>\n",
              "      <td>21.0</td>\n",
              "      <td>7099.0</td>\n",
              "      <td>1106.0</td>\n",
              "      <td>2401.0</td>\n",
              "      <td>1138.0</td>\n",
              "      <td>8.3014</td>\n",
              "      <td>358500.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-122.24</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1467.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>496.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>7.2574</td>\n",
              "      <td>352100.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-122.25</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1274.0</td>\n",
              "      <td>235.0</td>\n",
              "      <td>558.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>5.6431</td>\n",
              "      <td>341300.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-122.25</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1627.0</td>\n",
              "      <td>280.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>259.0</td>\n",
              "      <td>3.8462</td>\n",
              "      <td>342200.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   longitude  latitude  ...  median_house_value  ocean_proximity\n",
              "0    -122.23     37.88  ...            452600.0         NEAR BAY\n",
              "1    -122.22     37.86  ...            358500.0         NEAR BAY\n",
              "2    -122.24     37.85  ...            352100.0         NEAR BAY\n",
              "3    -122.25     37.85  ...            341300.0         NEAR BAY\n",
              "4    -122.25     37.85  ...            342200.0         NEAR BAY\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHZqgYbSWyRn"
      },
      "source": [
        "## Scraping the Web\n",
        "\n",
        "* Another way to get data is by scraping it from web pages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvwH-bv5XB-8"
      },
      "source": [
        "### HTML Parsing\n",
        "\n",
        "* To get data out of HTML, we will use the **Beautiful Soup** library, which builds a tree out of the various elements on a web page and provides a simple interface for accessing them.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1hCVNBHa05qbQyT87QlabhtH9let1c9pq)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgxM2d48RwTc"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# I put the relevant HTML file on GitHub. In order to fit\n",
        "# the URL in the book I had to split it across two lines.\n",
        "# Recall that whitespace-separated strings get concatenated.\n",
        "url = (\"https://raw.githubusercontent.com/\"\n",
        "       \"joelgrus/data/master/getting-data.html\")\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html5lib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcsu_R5-XYiW"
      },
      "source": [
        "* We'll typically work with Tag objects, which correspond to the tags representing the structure of an HTML page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XZ6UgluRwwE"
      },
      "source": [
        "first_paragraph = soup.find('p')        # or just soup.p\n",
        "\n",
        "assert str(soup.find('p')) == '<p id=\"p1\">This is the first paragraph.</p>'\n",
        "\n",
        "first_paragraph_text = soup.p.text\n",
        "first_paragraph_words = soup.p.text.split()\n",
        "\n",
        "assert first_paragraph_words == ['This', 'is', 'the', 'first', 'paragraph.']\n",
        "\n",
        "first_paragraph_id = soup.p['id']       # raises KeyError if no 'id'\n",
        "first_paragraph_id2 = soup.p.get('id')  # returns None if no 'id'\n",
        "\n",
        "assert first_paragraph_id == first_paragraph_id2 == 'p1'\n",
        "\n",
        "all_paragraphs = soup.find_all('p')  # or just soup('p')\n",
        "paragraphs_with_ids = [p for p in soup('p') if p.get('id')]\n",
        "\n",
        "assert len(all_paragraphs) == 2\n",
        "assert len(paragraphs_with_ids) == 1\n",
        "\n",
        "important_paragraphs = soup('p', {'class' : 'important'})\n",
        "important_paragraphs2 = soup('p', 'important')\n",
        "important_paragraphs3 = [p for p in soup('p')\n",
        "                         if 'important' in p.get('class', [])]\n",
        "\n",
        "assert important_paragraphs == important_paragraphs2 == important_paragraphs3\n",
        "assert len(important_paragraphs) == 1                         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9eUUu5BYEGY"
      },
      "source": [
        "* You can combine these methods to implement more elaborate logic.\n",
        ">* For example, if you want to find every `<span>` element that is contained inside a `<div>` element, you could do this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oOhNwJPT1V7"
      },
      "source": [
        "# warning, will return the same span multiple times\n",
        "# if it sits inside multiple divs\n",
        "# be more clever if that's the case\n",
        "spans_inside_divs = [span\n",
        "                     for div in soup('div')     # for each <div> on the page\n",
        "                     for span in div('span')]   # find each <span> inside it\n",
        "\n",
        "assert len(spans_inside_divs) == 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhgVYHQNZH79"
      },
      "source": [
        "### Example: Keeping Tabs on Congress\n",
        "\n",
        "* The VP of Policy at DataSciencester is worried about potential regulation of the data science industry and asks you to quantify what Congress is saying on the topic. In particular, he wants you to find all the representatives who have press releases about \"data.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_LMgmTEVpeL",
        "outputId": "9f72f031-7d02-45d4-df79-24b168c2aa39"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "    \n",
        "url = \"https://www.house.gov/representatives\"\n",
        "text = requests.get(url).text\n",
        "soup = BeautifulSoup(text, \"html5lib\")\n",
        "    \n",
        "all_urls = [a['href']\n",
        "            for a in soup('a')\n",
        "            if a.has_attr('href')]\n",
        "    \n",
        "print(len(all_urls))  # 965 for me, way too many "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNPDrJ1kcm4f"
      },
      "source": [
        "* If you look at them, the ones we want start with\n",
        "either *http://* or *https://*, have some kind of name, and end with either *.house.gov* or *.house.gov/*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJjThDyFYOKs",
        "outputId": "f52a135d-401a-4cb6-828d-086f743e2417"
      },
      "source": [
        "import re\n",
        "    \n",
        "# Must start with http:// or https://\n",
        "# Must end with .house.gov or .house.gov/\n",
        "regex = r\"^https?://.*\\.house\\.gov/?$\"\n",
        "    \n",
        "# Let's write some tests!\n",
        "assert re.match(regex, \"http://joel.house.gov\")\n",
        "assert re.match(regex, \"https://joel.house.gov\")\n",
        "assert re.match(regex, \"http://joel.house.gov/\")\n",
        "assert re.match(regex, \"https://joel.house.gov/\")\n",
        "assert not re.match(regex, \"joel.house.gov\")\n",
        "assert not re.match(regex, \"http://joel.house.com\")\n",
        "assert not re.match(regex, \"https://joel.house.gov/biography\")\n",
        "\n",
        "# And now apply\n",
        "good_urls = [url for url in all_urls if re.match(regex, url)]\n",
        "print(len(good_urls))  # still 862 for me"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "870\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBgnFGEucNLY"
      },
      "source": [
        "* If you look at the list, there are a lot of duplicates. Let's use `set` to get rid of them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb-YR8IJVpkw",
        "outputId": "f38da796-b48e-4816-ff0f-aaa5c4392868"
      },
      "source": [
        "num_original_good_urls = len(good_urls)\n",
        "good_urls = list(set(good_urls))\n",
        "print(len(good_urls))  # only 431 for me\n",
        "      \n",
        "assert len(good_urls) < num_original_good_urls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1H7Fm49dRQz"
      },
      "source": [
        "* When we look at the sites, most of them have a link to press releases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDEi7hFcVpqo",
        "outputId": "2d18f04a-b998-462d-997a-4d1c99c4718e"
      },
      "source": [
        "html = requests.get('https://jayapal.house.gov').text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n",
        "    \n",
        "# Use a set because the links might appear multiple times.\n",
        "links = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()}\n",
        "\n",
        "print(links) # {'/media/press-releases'}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'https://jayapal.house.gov/category/press-releases/', 'https://jayapal.house.gov/category/news/'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6K_ga0edXD3"
      },
      "source": [
        "* We'll write a slightly more general function that checks whether a page\n",
        "of press releases mentions any given term."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3zm3Ck-T1oK"
      },
      "source": [
        "def paragraph_mentions(text: str, keyword: str) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if a <p> inside the text mentions {keyword}\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(text, 'html5lib')\n",
        "    paragraphs = [p.get_text() for p in soup('p')]\n",
        "\n",
        "    return any(keyword.lower() in paragraph.lower()\n",
        "               for paragraph in paragraphs)\n",
        "\n",
        "text = \"\"\"<body><h1>Facebook</h1><p>Twitter</p>\"\"\"\n",
        "assert paragraph_mentions(text, \"twitter\")       # is inside a <p>\n",
        "assert not paragraph_mentions(text, \"facebook\")  # not inside a <p>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFDxOjM7VpwD",
        "outputId": "2213e3e8-5c8b-4f0c-f3cf-92701093da36"
      },
      "source": [
        "# I don't want this file to scrape all 400+ websites every time it runs.\n",
        "# So I'm going to randomly throw out most of the urls.\n",
        "# The code in the book doesn't do this.\n",
        "import random\n",
        "good_urls = random.sample(good_urls, 5)\n",
        "print(f\"after sampling, left with {good_urls}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "after sampling, left with ['https://upton.house.gov', 'https://baird.house.gov/', 'https://hartzler.house.gov/', 'https://gaetz.house.gov', 'https://lee.house.gov/']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y9vuHNMdosP"
      },
      "source": [
        "* At last we are re ready to find the relevant congresspeople."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRWmLd7sb4xd",
        "outputId": "4d8cff05-5518-447a-a83e-92dcd3ba1550"
      },
      "source": [
        "from typing import Dict, Set\n",
        "\n",
        "press_releases: Dict[str, Set[str]] = {}\n",
        "    \n",
        "for house_url in good_urls:\n",
        "    html = requests.get(house_url).text\n",
        "    soup = BeautifulSoup(html, 'html5lib')\n",
        "    pr_links = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()}\n",
        "    print(f\"{house_url}: {pr_links}\")\n",
        "    press_releases[house_url] = pr_links\n",
        "    \n",
        "for house_url, pr_links in press_releases.items():\n",
        "    for pr_link in pr_links:\n",
        "        url = f\"{house_url}/{pr_link}\"\n",
        "        text = requests.get(url).text\n",
        "   \n",
        "        if paragraph_mentions(text, 'data'):\n",
        "            print(f\"{house_url}\")\n",
        "            break  # done with this house_url"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://upton.house.gov: {'/News/DocumentQuery.aspx?DocumentTypeID=1828'}\n",
            "https://baird.house.gov/: {'/news/documentquery.aspx?DocumentTypeID=27'}\n",
            "https://hartzler.house.gov/: {'/media-center/press-releases'}\n",
            "https://gaetz.house.gov: {'/media/press-releases'}\n",
            "https://lee.house.gov/: set()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgotknUedtEO"
      },
      "source": [
        "## Using APIs\n",
        "\n",
        "* Many websites and web services provide application programming interfaces (APIs), which allow you to explicitly request data in a structured format. This saves you the trouble of having to scrape them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sRhSqqadzhN"
      },
      "source": [
        "### JSON and XML\n",
        "\n",
        "* Because HTTP is a protocol for transferring *text*, the data you request through a web API needs to be **serialized** into a string format. Often this serialization uses **JavaScript Object Notation (JSON)**.\n",
        "* JavaScript objects look quite similar to Python `dict`, which\n",
        "makes their string representations easy to interpret:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1tDJs_dfskw",
        "outputId": "f6e1bff0-fbeb-43f4-b3ef-37cd9189cb02"
      },
      "source": [
        "{ \"title\" : \"Data Science Book\",\n",
        "  \"author\" : \"Joel Grus\",\n",
        "  \"publicationYear\" : 2019,\n",
        "  \"topics\" : [ \"data\", \"science\", \"data science\"] }"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'author': 'Joel Grus',\n",
              " 'publicationYear': 2019,\n",
              " 'title': 'Data Science Book',\n",
              " 'topics': ['data', 'science', 'data science']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjuL3yssfmv6"
      },
      "source": [
        "* We can parse JSON using Python's `json` module. In particular, we will use its `loads` function, which deserializes a string representing a JSON object into a Python object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NFbh6y3VpPU"
      },
      "source": [
        "import json\n",
        "serialized = \"\"\"{ \"title\" : \"Data Science Book\",\n",
        "                  \"author\" : \"Joel Grus\",\n",
        "                  \"publicationYear\" : 2019,\n",
        "                  \"topics\" : [ \"data\", \"science\", \"data science\"] }\"\"\"\n",
        "\n",
        "# parse the JSON to create a Python dict\n",
        "deserialized = json.loads(serialized)\n",
        "assert deserialized[\"publicationYear\"] == 2019\n",
        "assert \"data science\" in deserialized[\"topics\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3AC-UMbfx49"
      },
      "source": [
        "### Using an Unauthenticated API\n",
        "\n",
        "* Most APIs these days require that you first authenticate yourself before you can use them. Accordingly, we'll start by taking a look at [GitHub's API](https://docs.github.com/en/rest), with which you can do some simple things unauthenticated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3EQ3AZDbmJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1c96b92-1a88-4f03-fb1b-a74dd280c5b0"
      },
      "source": [
        "import requests, json\n",
        "    \n",
        "github_user = \"joelgrus\"\n",
        "endpoint = f\"https://api.github.com/users/{github_user}/repos\"\n",
        "    \n",
        "repos = json.loads(requests.get(endpoint).text)\n",
        "    \n",
        "from collections import Counter\n",
        "from dateutil.parser import parse\n",
        "    \n",
        "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
        "month_counts = Counter(date.month for date in dates)\n",
        "weekday_counts = Counter(date.weekday() for date in dates)\n",
        "    \n",
        "last_5_repositories = sorted(repos,\n",
        "                             key=lambda r: r[\"pushed_at\"],\n",
        "                             reverse=True)[:5]\n",
        "print(last_5_repositories)\n",
        "    \n",
        "last_5_languages = [repo[\"language\"]\n",
        "                    for repo in last_5_repositories]\n",
        "print(last_5_languages)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'id': 26382146, 'node_id': 'MDEwOlJlcG9zaXRvcnkyNjM4MjE0Ng==', 'name': 'data-science-from-scratch', 'full_name': 'joelgrus/data-science-from-scratch', 'private': False, 'owner': {'login': 'joelgrus', 'id': 1308313, 'node_id': 'MDQ6VXNlcjEzMDgzMTM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1308313?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/joelgrus', 'html_url': 'https://github.com/joelgrus', 'followers_url': 'https://api.github.com/users/joelgrus/followers', 'following_url': 'https://api.github.com/users/joelgrus/following{/other_user}', 'gists_url': 'https://api.github.com/users/joelgrus/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/joelgrus/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/joelgrus/subscriptions', 'organizations_url': 'https://api.github.com/users/joelgrus/orgs', 'repos_url': 'https://api.github.com/users/joelgrus/repos', 'events_url': 'https://api.github.com/users/joelgrus/events{/privacy}', 'received_events_url': 'https://api.github.com/users/joelgrus/received_events', 'type': 'User', 'site_admin': False}, 'html_url': 'https://github.com/joelgrus/data-science-from-scratch', 'description': 'code for Data Science From Scratch book', 'fork': False, 'url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch', 'forks_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/forks', 'keys_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/teams', 'hooks_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/hooks', 'issue_events_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/issues/events{/number}', 'events_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/events', 'assignees_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/assignees{/user}', 'branches_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/branches{/branch}', 'tags_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/tags', 'blobs_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/languages', 'stargazers_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/stargazers', 'contributors_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/contributors', 'subscribers_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/subscribers', 'subscription_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/subscription', 'commits_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/contents/{+path}', 'compare_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/merges', 'archive_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/downloads', 'issues_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/issues{/number}', 'pulls_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/labels{/name}', 'releases_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/releases{/id}', 'deployments_url': 'https://api.github.com/repos/joelgrus/data-science-from-scratch/deployments', 'created_at': '2014-11-09T02:31:24Z', 'updated_at': '2021-03-29T21:10:33Z', 'pushed_at': '2021-03-08T20:05:10Z', 'git_url': 'git://github.com/joelgrus/data-science-from-scratch.git', 'ssh_url': 'git@github.com:joelgrus/data-science-from-scratch.git', 'clone_url': 'https://github.com/joelgrus/data-science-from-scratch.git', 'svn_url': 'https://github.com/joelgrus/data-science-from-scratch', 'homepage': None, 'size': 769, 'stargazers_count': 5874, 'watchers_count': 5874, 'language': 'Python', 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': False, 'forks_count': 3397, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 67, 'license': {'key': 'mit', 'name': 'MIT License', 'spdx_id': 'MIT', 'url': 'https://api.github.com/licenses/mit', 'node_id': 'MDc6TGljZW5zZTEz'}, 'forks': 3397, 'open_issues': 67, 'watchers': 5874, 'default_branch': 'master'}, {'id': 314853168, 'node_id': 'MDEwOlJlcG9zaXRvcnkzMTQ4NTMxNjg=', 'name': 'advent2020', 'full_name': 'joelgrus/advent2020', 'private': False, 'owner': {'login': 'joelgrus', 'id': 1308313, 'node_id': 'MDQ6VXNlcjEzMDgzMTM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1308313?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/joelgrus', 'html_url': 'https://github.com/joelgrus', 'followers_url': 'https://api.github.com/users/joelgrus/followers', 'following_url': 'https://api.github.com/users/joelgrus/following{/other_user}', 'gists_url': 'https://api.github.com/users/joelgrus/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/joelgrus/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/joelgrus/subscriptions', 'organizations_url': 'https://api.github.com/users/joelgrus/orgs', 'repos_url': 'https://api.github.com/users/joelgrus/repos', 'events_url': 'https://api.github.com/users/joelgrus/events{/privacy}', 'received_events_url': 'https://api.github.com/users/joelgrus/received_events', 'type': 'User', 'site_admin': False}, 'html_url': 'https://github.com/joelgrus/advent2020', 'description': 'advent of code 2020', 'fork': False, 'url': 'https://api.github.com/repos/joelgrus/advent2020', 'forks_url': 'https://api.github.com/repos/joelgrus/advent2020/forks', 'keys_url': 'https://api.github.com/repos/joelgrus/advent2020/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/joelgrus/advent2020/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/joelgrus/advent2020/teams', 'hooks_url': 'https://api.github.com/repos/joelgrus/advent2020/hooks', 'issue_events_url': 'https://api.github.com/repos/joelgrus/advent2020/issues/events{/number}', 'events_url': 'https://api.github.com/repos/joelgrus/advent2020/events', 'assignees_url': 'https://api.github.com/repos/joelgrus/advent2020/assignees{/user}', 'branches_url': 'https://api.github.com/repos/joelgrus/advent2020/branches{/branch}', 'tags_url': 'https://api.github.com/repos/joelgrus/advent2020/tags', 'blobs_url': 'https://api.github.com/repos/joelgrus/advent2020/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/joelgrus/advent2020/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/joelgrus/advent2020/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/joelgrus/advent2020/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/joelgrus/advent2020/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/joelgrus/advent2020/languages', 'stargazers_url': 'https://api.github.com/repos/joelgrus/advent2020/stargazers', 'contributors_url': 'https://api.github.com/repos/joelgrus/advent2020/contributors', 'subscribers_url': 'https://api.github.com/repos/joelgrus/advent2020/subscribers', 'subscription_url': 'https://api.github.com/repos/joelgrus/advent2020/subscription', 'commits_url': 'https://api.github.com/repos/joelgrus/advent2020/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/joelgrus/advent2020/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/joelgrus/advent2020/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/joelgrus/advent2020/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/joelgrus/advent2020/contents/{+path}', 'compare_url': 'https://api.github.com/repos/joelgrus/advent2020/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/joelgrus/advent2020/merges', 'archive_url': 'https://api.github.com/repos/joelgrus/advent2020/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/joelgrus/advent2020/downloads', 'issues_url': 'https://api.github.com/repos/joelgrus/advent2020/issues{/number}', 'pulls_url': 'https://api.github.com/repos/joelgrus/advent2020/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/joelgrus/advent2020/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/joelgrus/advent2020/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/joelgrus/advent2020/labels{/name}', 'releases_url': 'https://api.github.com/repos/joelgrus/advent2020/releases{/id}', 'deployments_url': 'https://api.github.com/repos/joelgrus/advent2020/deployments', 'created_at': '2020-11-21T16:21:49Z', 'updated_at': '2021-02-02T13:56:03Z', 'pushed_at': '2020-12-25T05:14:47Z', 'git_url': 'git://github.com/joelgrus/advent2020.git', 'ssh_url': 'git@github.com:joelgrus/advent2020.git', 'clone_url': 'https://github.com/joelgrus/advent2020.git', 'svn_url': 'https://github.com/joelgrus/advent2020', 'homepage': None, 'size': 146, 'stargazers_count': 27, 'watchers_count': 27, 'language': 'Python', 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': False, 'forks_count': 4, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 0, 'license': None, 'forks': 4, 'open_issues': 0, 'watchers': 27, 'default_branch': 'master'}, {'id': 298829523, 'node_id': 'MDEwOlJlcG9zaXRvcnkyOTg4Mjk1MjM=', 'name': 'collaborative-regression', 'full_name': 'joelgrus/collaborative-regression', 'private': False, 'owner': {'login': 'joelgrus', 'id': 1308313, 'node_id': 'MDQ6VXNlcjEzMDgzMTM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1308313?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/joelgrus', 'html_url': 'https://github.com/joelgrus', 'followers_url': 'https://api.github.com/users/joelgrus/followers', 'following_url': 'https://api.github.com/users/joelgrus/following{/other_user}', 'gists_url': 'https://api.github.com/users/joelgrus/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/joelgrus/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/joelgrus/subscriptions', 'organizations_url': 'https://api.github.com/users/joelgrus/orgs', 'repos_url': 'https://api.github.com/users/joelgrus/repos', 'events_url': 'https://api.github.com/users/joelgrus/events{/privacy}', 'received_events_url': 'https://api.github.com/users/joelgrus/received_events', 'type': 'User', 'site_admin': False}, 'html_url': 'https://github.com/joelgrus/collaborative-regression', 'description': 'collaborative regression', 'fork': False, 'url': 'https://api.github.com/repos/joelgrus/collaborative-regression', 'forks_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/forks', 'keys_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/teams', 'hooks_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/hooks', 'issue_events_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/issues/events{/number}', 'events_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/events', 'assignees_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/assignees{/user}', 'branches_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/branches{/branch}', 'tags_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/tags', 'blobs_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/languages', 'stargazers_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/stargazers', 'contributors_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/contributors', 'subscribers_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/subscribers', 'subscription_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/subscription', 'commits_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/contents/{+path}', 'compare_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/merges', 'archive_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/downloads', 'issues_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/issues{/number}', 'pulls_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/labels{/name}', 'releases_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/releases{/id}', 'deployments_url': 'https://api.github.com/repos/joelgrus/collaborative-regression/deployments', 'created_at': '2020-09-26T14:19:22Z', 'updated_at': '2020-11-28T18:53:11Z', 'pushed_at': '2020-11-28T18:53:09Z', 'git_url': 'git://github.com/joelgrus/collaborative-regression.git', 'ssh_url': 'git@github.com:joelgrus/collaborative-regression.git', 'clone_url': 'https://github.com/joelgrus/collaborative-regression.git', 'svn_url': 'https://github.com/joelgrus/collaborative-regression', 'homepage': None, 'size': 4, 'stargazers_count': 0, 'watchers_count': 0, 'language': 'Python', 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': False, 'forks_count': 1, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 0, 'license': {'key': 'mit', 'name': 'MIT License', 'spdx_id': 'MIT', 'url': 'https://api.github.com/licenses/mit', 'node_id': 'MDc6TGljZW5zZTEz'}, 'forks': 1, 'open_issues': 0, 'watchers': 0, 'default_branch': 'master'}, {'id': 33576630, 'node_id': 'MDEwOlJlcG9zaXRvcnkzMzU3NjYzMA==', 'name': 'drunken-avenger', 'full_name': 'joelgrus/drunken-avenger', 'private': False, 'owner': {'login': 'joelgrus', 'id': 1308313, 'node_id': 'MDQ6VXNlcjEzMDgzMTM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1308313?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/joelgrus', 'html_url': 'https://github.com/joelgrus', 'followers_url': 'https://api.github.com/users/joelgrus/followers', 'following_url': 'https://api.github.com/users/joelgrus/following{/other_user}', 'gists_url': 'https://api.github.com/users/joelgrus/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/joelgrus/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/joelgrus/subscriptions', 'organizations_url': 'https://api.github.com/users/joelgrus/orgs', 'repos_url': 'https://api.github.com/users/joelgrus/repos', 'events_url': 'https://api.github.com/users/joelgrus/events{/privacy}', 'received_events_url': 'https://api.github.com/users/joelgrus/received_events', 'type': 'User', 'site_admin': False}, 'html_url': 'https://github.com/joelgrus/drunken-avenger', 'description': 'the pelican setup for my blog, as well as all the content', 'fork': False, 'url': 'https://api.github.com/repos/joelgrus/drunken-avenger', 'forks_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/forks', 'keys_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/teams', 'hooks_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/hooks', 'issue_events_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/issues/events{/number}', 'events_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/events', 'assignees_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/assignees{/user}', 'branches_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/branches{/branch}', 'tags_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/tags', 'blobs_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/languages', 'stargazers_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/stargazers', 'contributors_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/contributors', 'subscribers_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/subscribers', 'subscription_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/subscription', 'commits_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/contents/{+path}', 'compare_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/merges', 'archive_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/downloads', 'issues_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/issues{/number}', 'pulls_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/labels{/name}', 'releases_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/releases{/id}', 'deployments_url': 'https://api.github.com/repos/joelgrus/drunken-avenger/deployments', 'created_at': '2015-04-08T01:01:47Z', 'updated_at': '2020-10-14T17:34:37Z', 'pushed_at': '2020-10-02T15:47:45Z', 'git_url': 'git://github.com/joelgrus/drunken-avenger.git', 'ssh_url': 'git@github.com:joelgrus/drunken-avenger.git', 'clone_url': 'https://github.com/joelgrus/drunken-avenger.git', 'svn_url': 'https://github.com/joelgrus/drunken-avenger', 'homepage': None, 'size': 13731, 'stargazers_count': 0, 'watchers_count': 0, 'language': 'JavaScript', 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': False, 'forks_count': 0, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 0, 'license': None, 'forks': 0, 'open_issues': 0, 'watchers': 0, 'default_branch': 'master'}, {'id': 225098708, 'node_id': 'MDEwOlJlcG9zaXRvcnkyMjUwOTg3MDg=', 'name': 'advent2019', 'full_name': 'joelgrus/advent2019', 'private': False, 'owner': {'login': 'joelgrus', 'id': 1308313, 'node_id': 'MDQ6VXNlcjEzMDgzMTM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1308313?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/joelgrus', 'html_url': 'https://github.com/joelgrus', 'followers_url': 'https://api.github.com/users/joelgrus/followers', 'following_url': 'https://api.github.com/users/joelgrus/following{/other_user}', 'gists_url': 'https://api.github.com/users/joelgrus/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/joelgrus/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/joelgrus/subscriptions', 'organizations_url': 'https://api.github.com/users/joelgrus/orgs', 'repos_url': 'https://api.github.com/users/joelgrus/repos', 'events_url': 'https://api.github.com/users/joelgrus/events{/privacy}', 'received_events_url': 'https://api.github.com/users/joelgrus/received_events', 'type': 'User', 'site_admin': False}, 'html_url': 'https://github.com/joelgrus/advent2019', 'description': 'advent of code 2019', 'fork': False, 'url': 'https://api.github.com/repos/joelgrus/advent2019', 'forks_url': 'https://api.github.com/repos/joelgrus/advent2019/forks', 'keys_url': 'https://api.github.com/repos/joelgrus/advent2019/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/joelgrus/advent2019/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/joelgrus/advent2019/teams', 'hooks_url': 'https://api.github.com/repos/joelgrus/advent2019/hooks', 'issue_events_url': 'https://api.github.com/repos/joelgrus/advent2019/issues/events{/number}', 'events_url': 'https://api.github.com/repos/joelgrus/advent2019/events', 'assignees_url': 'https://api.github.com/repos/joelgrus/advent2019/assignees{/user}', 'branches_url': 'https://api.github.com/repos/joelgrus/advent2019/branches{/branch}', 'tags_url': 'https://api.github.com/repos/joelgrus/advent2019/tags', 'blobs_url': 'https://api.github.com/repos/joelgrus/advent2019/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/joelgrus/advent2019/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/joelgrus/advent2019/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/joelgrus/advent2019/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/joelgrus/advent2019/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/joelgrus/advent2019/languages', 'stargazers_url': 'https://api.github.com/repos/joelgrus/advent2019/stargazers', 'contributors_url': 'https://api.github.com/repos/joelgrus/advent2019/contributors', 'subscribers_url': 'https://api.github.com/repos/joelgrus/advent2019/subscribers', 'subscription_url': 'https://api.github.com/repos/joelgrus/advent2019/subscription', 'commits_url': 'https://api.github.com/repos/joelgrus/advent2019/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/joelgrus/advent2019/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/joelgrus/advent2019/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/joelgrus/advent2019/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/joelgrus/advent2019/contents/{+path}', 'compare_url': 'https://api.github.com/repos/joelgrus/advent2019/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/joelgrus/advent2019/merges', 'archive_url': 'https://api.github.com/repos/joelgrus/advent2019/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/joelgrus/advent2019/downloads', 'issues_url': 'https://api.github.com/repos/joelgrus/advent2019/issues{/number}', 'pulls_url': 'https://api.github.com/repos/joelgrus/advent2019/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/joelgrus/advent2019/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/joelgrus/advent2019/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/joelgrus/advent2019/labels{/name}', 'releases_url': 'https://api.github.com/repos/joelgrus/advent2019/releases{/id}', 'deployments_url': 'https://api.github.com/repos/joelgrus/advent2019/deployments', 'created_at': '2019-12-01T02:57:18Z', 'updated_at': '2020-09-10T18:16:05Z', 'pushed_at': '2019-12-25T06:34:04Z', 'git_url': 'git://github.com/joelgrus/advent2019.git', 'ssh_url': 'git@github.com:joelgrus/advent2019.git', 'clone_url': 'https://github.com/joelgrus/advent2019.git', 'svn_url': 'https://github.com/joelgrus/advent2019', 'homepage': None, 'size': 96, 'stargazers_count': 21, 'watchers_count': 21, 'language': 'Python', 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': False, 'forks_count': 3, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 0, 'license': None, 'forks': 3, 'open_issues': 0, 'watchers': 21, 'default_branch': 'master'}]\n",
            "['Python', 'Python', 'Python', 'JavaScript', 'Python']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNIEEk3-geOL"
      },
      "source": [
        "### Finding APIs\n",
        "\n",
        "* There are libraries for the Yelp API, for the Instagram API, for the Spotify API, and so on.\n",
        "* If you're looking for a list of APIs that have Python wrappers, there's a nice one from [Real Python on GitHub](https://docs.github.com/en/rest)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjkzxXvFgtug"
      },
      "source": [
        "## Example: Using the Twitter APIs\n",
        "\n",
        "* **Twitter** is a fantastic source of data to work with.\n",
        "* You can use it to get real-time news. You can use it to measure reactions to current events. You can use it to find links related to specific topics. You can use it for pretty much anything you can imagine, just as long as you can get access to its data. And you can get access to its data through its APIs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WxMdvHqg7xe"
      },
      "source": [
        "### Getting Credentials\n",
        "\n",
        "Here are the steps:\n",
        "1. Go to https://developer.twitter.com/.\n",
        "2. If you are not signed in, click \"Sign in\" and enter your Twitter username and\n",
        "password.\n",
        "3. Click `Apply` to apply for a developer account.\n",
        "4. Request access for your own personal use.\n",
        "5. Fill out the application. It requires 300 words (really) on why you need access, so to get over the limit you could tell them about this book and how much you're enjoying it.\n",
        "6. Wait some indefinite amount of time.\n",
        "7. Once you get approved, go back to developer.twitter.com, find the \"Apps\" section, and click \"Create an app.\"\n",
        "8. Fill out all the required fields (again, if you need extra characters for the description, you could talk about this book and how edifying you're finding it).\n",
        "9. Click `Create`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xESAmUOWZ7BX"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1wTREQhuNaSUhZMuk8rs76nVkL0NKl9Yt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE3ucQzxqHh_"
      },
      "source": [
        "* Now your app should have a \"Keys and tokens\" tab with a \"Consumer API keys\" section that lists an \"API key\" and an \"API secret key.\" Take note of those keys; you'll need them. (Also, keep them secret! They're like passwords.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcQPCT9-aOOk"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1T1v6Y7MBrcf2VFm95-Zg5FrUbq9dftzl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPdYQHsgisBj"
      },
      "source": [
        "### Using Twython\n",
        "\n",
        "* The trickiest part of using the Twitter API is authenticating yourself. API providers want to make sure that you're authorized to access their data and that you don't exceed their usage limits. They also want to know who's accessing their data.\n",
        "* There is a simple way, OAuth 2, that suffices when you just want to do simple searches. And there is a complex way, OAuth 1, that's required when you want to perform actions (e.g., tweeting) or (in particular for us) connect to the Twitter stream.\n",
        "* So we're stuck with the more complicated way, which we'll try to automate as much as we can."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibFTEyBFjwEv"
      },
      "source": [
        "* First, you need your API key and API secret key (sometimes known as the consumer key and consumer secret, respectively)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dpIK8glj5J9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9214b0b9-4ab3-453b-f949-cc86a8ddf688"
      },
      "source": [
        "# Feel free to plug your key and secret in directly\n",
        "credentials = {}  \n",
        "# Your credentials\n",
        "credentials['CONSUMER_KEY'] = #\n",
        "credentials['CONSUMER_SECRET'] = #\n",
        "# credentials['ACCESS_TOKEN'] = #\n",
        "# credentials['ACCESS_SECRET'] = #\n",
        "\n",
        "CONSUMER_KEY = credentials['CONSUMER_KEY']\n",
        "CONSUMER_SECRET = credentials['CONSUMER_SECRET']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-2c60a653242a>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    credentials['CONSUMER_KEY'] = #\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mc_ktVSj6lS"
      },
      "source": [
        "* Now we can instantiate the client:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOVw5JWVbmap"
      },
      "source": [
        "import webbrowser\n",
        "!pip install twython\n",
        "# Import the Twython class\n",
        "from twython import Twython\n",
        "    \n",
        "# Get a temporary client to retrieve an authentication url\n",
        "temp_client = Twython(CONSUMER_KEY, CONSUMER_SECRET)\n",
        "temp_creds = temp_client.get_authentication_tokens()\n",
        "url = temp_creds['auth_url']\n",
        "    \n",
        "# Now visit that URL to authorize the application and get a PIN\n",
        "print(f\"go visit {url} and get the PIN code and paste it below\")\n",
        "webbrowser.open(url)\n",
        "PIN_CODE = input(\"please enter the PIN code: \")\n",
        "    \n",
        "# Now we use that PIN_CODE to get the actual tokens\n",
        "auth_client = Twython(CONSUMER_KEY,\n",
        "                      CONSUMER_SECRET,\n",
        "                      temp_creds['oauth_token'],\n",
        "                      temp_creds['oauth_token_secret'])\n",
        "final_step = auth_client.get_authorized_tokens(PIN_CODE)\n",
        "ACCESS_TOKEN = final_step['oauth_token']\n",
        "ACCESS_TOKEN_SECRET = final_step['oauth_token_secret']\n",
        "    \n",
        "# And get a new Twython instance using them.\n",
        "twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqBWgBQjkoYo"
      },
      "source": [
        "* The [Streaming API](https://developer.twitter.com/en/docs/tutorials/consuming-streaming-data) allows you to connect to (a sample of) the great Twitter firehose. \n",
        "* To use it, you'll need to authenticate using your access tokens. \n",
        "* In order to access the Streaming API with `Twython`, we need to define a class that inherits from `TwythonStreamer` and that overrides its `on_success` method, and possibly its `on_error method`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GjiQ0ftmgZE"
      },
      "source": [
        "* `MyStreamer` will connect to the Twitter stream and wait for Twitter to feed it data. Each time it receives some data (here, a tweet represented as a Python object), it passes it to the `on_success` method, which appends it to our `tweets` list if its language is English, and then disconnects the streamer after it's collected 1,000 tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0GLtMBfmvrX"
      },
      "source": [
        "from twython import TwythonStreamer\n",
        "    \n",
        "# Appending data to a global variable is pretty poor form\n",
        "# but it makes the example much simpler\n",
        "tweets = []\n",
        "    \n",
        "class MyStreamer(TwythonStreamer):\n",
        "    def on_success(self, data):\n",
        "        \"\"\"\n",
        "        What do we do when twitter sends us data?\n",
        "        Here data will be a Python dict representing a tweet\n",
        "        \"\"\"\n",
        "        # We only want to collect English-language tweets\n",
        "        if data.get('lang') == 'en':\n",
        "            tweets.append(data)\n",
        "            print(f\"received tweet #{len(tweets)}\")\n",
        "\n",
        "        # Stop when we've collected enough\n",
        "        if len(tweets) >= 100:\n",
        "            self.disconnect()\n",
        "    \n",
        "    def on_error(self, status_code, data):\n",
        "        print(status_code, data)\n",
        "        self.disconnect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP5pC9JYRxKp"
      },
      "source": [
        "stream = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET,\n",
        "                    ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
        "    \n",
        "# starts consuming public statuses that contain the keyword 'data'\n",
        "stream.statuses.filter(track='data')\n",
        "    \n",
        "# if instead we wanted to start consuming a sample of *all* public statuses\n",
        "# stream.statuses.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ITt3No2mOgk"
      },
      "source": [
        "* This will run until it collects 100 tweets (or until it encounters an error) and stop, at which point you can start analyzing those tweets.\n",
        "* For instance, you could find the most common hashtags with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBNOV_Brm-c0"
      },
      "source": [
        "top_hashtags = Counter(hashtag['text'].lower()\n",
        "                       for tweet in tweets\n",
        "                       for hashtag in tweet[\"entities\"][\"hashtags\"])\n",
        "\n",
        "print(top_hashtags.most_common(5))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}