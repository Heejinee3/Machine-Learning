{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mxzWG0juqnC"
      },
      "source": [
        "# **Chapter 18. Neural Networks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66K_mS1WI1Fm"
      },
      "source": [
        "#@title\n",
        "\n",
        "import requests\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/joelgrus/data-science-from-scratch/master/scratch/linear_algebra.py'\n",
        "r = requests.get(url)\n",
        "\n",
        "with open('linear_algebra.py', 'w') as f:\n",
        "    f.write(r.text)\n",
        "\n",
        "from linear_algebra import Vector, dot, add, squared_distance, scalar_multiply\n",
        "\n",
        "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
        "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
        "    assert len(v) == len(gradient)\n",
        "    step = scalar_multiply(step_size, gradient)\n",
        "    return add(v, step)\n",
        "\n",
        "def predict(x: Vector, beta: Vector) -> float:\n",
        "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
        "    return dot(x, beta)\n",
        "    \n",
        "def error(x: Vector, y: float, beta: Vector) -> float:\n",
        "    return predict(x, beta) - y\n",
        "\n",
        "def squared_error(x: Vector, y: float, beta: Vector) -> float:\n",
        "    return error(x, y, beta) ** 2\n",
        "\n",
        "def sqerror_gradient(x: Vector, y: float, beta: Vector) -> Vector:\n",
        "    err = error(x, y, beta)\n",
        "    return [2 * err * x_i for x_i in x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIahOFGP1zsV"
      },
      "source": [
        "## **Perceptrons**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3AHlDaD1xkl"
      },
      "source": [
        "def step_function(x: float) -> float:\n",
        "    return 1.0 if x >= 0 else 0.0\n",
        "\n",
        "def perceptron_output(weights: Vector, bias: float, x: Vector) -> float:\n",
        "    \"\"\"Returns 1 if the perceptron 'fires', 0 if not\"\"\"\n",
        "    calculation = dot(weights, x) + bias\n",
        "    return step_function(calculation)\n",
        "\n",
        "and_weights = [2., 2]\n",
        "and_bias = -3.\n",
        "\n",
        "assert perceptron_output(and_weights, and_bias, [1, 1]) == 1\n",
        "assert perceptron_output(and_weights, and_bias, [0, 1]) == 0\n",
        "assert perceptron_output(and_weights, and_bias, [1, 0]) == 0\n",
        "assert perceptron_output(and_weights, and_bias, [0, 0]) == 0\n",
        "\n",
        "or_weights = [2., 2]\n",
        "or_bias = -1.\n",
        "\n",
        "assert perceptron_output(or_weights, or_bias, [1, 1]) == 1\n",
        "assert perceptron_output(or_weights, or_bias, [0, 1]) == 1\n",
        "assert perceptron_output(or_weights, or_bias, [1, 0]) == 1\n",
        "assert perceptron_output(or_weights, or_bias, [0, 0]) == 0\n",
        "\n",
        "not_weights = [-2.]\n",
        "not_bias = 1.\n",
        "\n",
        "assert perceptron_output(not_weights, not_bias, [0]) == 1\n",
        "assert perceptron_output(not_weights, not_bias, [1]) == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJoUZl8G15xN"
      },
      "source": [
        "## **Feed-Forward Neural Networks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPmFJYqyIk_1"
      },
      "source": [
        "import math\n",
        "\n",
        "def sigmoid(t: float) -> float:\n",
        "    return 1 / (1 + math.exp(-t))\n",
        "\n",
        "def neuron_output(weights: Vector, inputs: Vector) -> float:\n",
        "    # weights includes the bias term, inputs includes a 1\n",
        "    return sigmoid(dot(weights, inputs))\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def feed_forward(neural_network: List[List[Vector]],\n",
        "                 input_vector: Vector) -> List[Vector]:\n",
        "    \"\"\"\n",
        "    Feeds the input vector through the neural network.\n",
        "    Returns the outputs of all layers (not just the last one).\n",
        "    \"\"\"\n",
        "    outputs: List[Vector] = []\n",
        "\n",
        "    for layer in neural_network:\n",
        "        input_with_bias = input_vector + [1]              # Add a constant.\n",
        "        output = [neuron_output(neuron, input_with_bias)  # Compute the output\n",
        "                  for neuron in layer]                    # for each neuron.\n",
        "        outputs.append(output)                            # Add to results.\n",
        "\n",
        "        # Then the input to the next layer is the output of this one\n",
        "        input_vector = output\n",
        "\n",
        "    return outputs\n",
        "\n",
        "xor_network = [# hidden layer\n",
        "               [[20., 20, -30],      # 'and' neuron\n",
        "                [20., 20, -10]],     # 'or'  neuron\n",
        "               # output layer\n",
        "               [[-60., 60, -30]]]    # '2nd input but not 1st input' neuron\n",
        "\n",
        "# feed_forward returns the outputs of all layers, so the [-1] gets the\n",
        "# final output, and the [0] gets the value out of the resulting vector\n",
        "assert 0.000 < feed_forward(xor_network, [0, 0])[-1][0] < 0.001\n",
        "assert 0.999 < feed_forward(xor_network, [1, 0])[-1][0] < 1.000\n",
        "assert 0.999 < feed_forward(xor_network, [0, 1])[-1][0] < 1.000\n",
        "assert 0.000 < feed_forward(xor_network, [1, 1])[-1][0] < 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9hPgkAKUj81"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1R2dbzvHwzpm4XSVO1v5EIFGQRH1k7ymQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mamBNiVk3z7S"
      },
      "source": [
        "## **Backpropagation**\n",
        "\n",
        "* We adjust the weights using the following algorithm:\n",
        "\n",
        ">1. Run `feed_forward` on an input vector to produce the outputs of all the neurons in the network.\n",
        ">2. We know the target output, so we can compute a loss that's the sum of the\n",
        "squared errors.\n",
        ">3. Compute the gradient of this loss as a function of the output neuron's weights.\n",
        ">4. \"Propagate\" the gradients and errors backward to compute the gradients with\n",
        "respect to the hidden neurons' weights.\n",
        ">5. Take a gradient descent step.\n",
        "\n",
        "* Typically we run this algorithm many times for our entire training set until the network converges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rZT9Hjd0g47",
        "outputId": "6910a455-c889-497b-ecc7-e6df8fd45e9c"
      },
      "source": [
        "def sqerror_gradients(network: List[List[Vector]],\n",
        "                      input_vector: Vector,\n",
        "                      target_vector: Vector) -> List[List[Vector]]:\n",
        "    \"\"\"\n",
        "    Given a neural network, an input vector, and a target vector,\n",
        "    make a prediction and compute the gradient of the squared error\n",
        "    loss with respect to the neuron weights.\n",
        "    \"\"\"\n",
        "    # forward pass\n",
        "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
        "\n",
        "    # gradients with respect to output neuron pre-activation outputs\n",
        "    output_deltas = [output * (1 - output) * (output - target)\n",
        "                     for output, target in zip(outputs, target_vector)]\n",
        "\n",
        "    # gradients with respect to output neuron weights\n",
        "    output_grads = [[output_deltas[i] * hidden_output\n",
        "                     for hidden_output in hidden_outputs + [1]]\n",
        "                    for i, output_neuron in enumerate(network[-1])]\n",
        "\n",
        "    # gradients with respect to hidden neuron pre-activation outputs\n",
        "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
        "                         dot(output_deltas, [n[i] for n in network[-1]])\n",
        "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
        "\n",
        "    # gradients with respect to hidden neuron weights\n",
        "    hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]]\n",
        "                    for i, hidden_neuron in enumerate(network[0])]\n",
        "\n",
        "    return [hidden_grads, output_grads]\n",
        "\n",
        "import random\n",
        "random.seed(0)\n",
        "    \n",
        "# training data\n",
        "xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
        "ys = [[0.], [1.], [1.], [0.]]\n",
        "    \n",
        "# start with random weights\n",
        "network = [ # hidden layer: 2 inputs -> 2 outputs\n",
        "            [[random.random() for _ in range(2 + 1)],   # 1st hidden neuron\n",
        "             [random.random() for _ in range(2 + 1)]],  # 2nd hidden neuron\n",
        "            # output layer: 2 inputs -> 1 output\n",
        "            [[random.random() for _ in range(2 + 1)]]   # 1st output neuron\n",
        "          ]\n",
        "    \n",
        "import tqdm\n",
        "    \n",
        "learning_rate = 1.0\n",
        "\n",
        "for epoch in tqdm.trange(20000, desc=\"neural net for xor\"):\n",
        "    for x, y in zip(xs, ys):\n",
        "        gradients = sqerror_gradients(network, x, y)\n",
        "    \n",
        "        # Take a gradient step for each neuron in each layer\n",
        "        network = [[gradient_step(neuron, grad, -learning_rate)\n",
        "                    for neuron, grad in zip(layer, layer_grad)]\n",
        "                   for layer, layer_grad in zip(network, gradients)]\n",
        "    \n",
        "# check that it learned XOR\n",
        "assert feed_forward(network, [0, 0])[-1][0] < 0.01\n",
        "assert feed_forward(network, [0, 1])[-1][0] > 0.99\n",
        "assert feed_forward(network, [1, 0])[-1][0] > 0.99\n",
        "assert feed_forward(network, [1, 1])[-1][0] < 0.01\n",
        "\n",
        "[   # hidden layer\n",
        "    [[7, 7, -3],     # computes OR\n",
        "     [5, 5, -8]],    # computes AND\n",
        "    # output layer\n",
        "    [[11, -12, -5]]  # computes \"first but not second\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "neural net for xor:   0%|          | 0/20000 [00:00<?, ?it/s]\u001b[A\n",
            "neural net for xor:   5%|▌         | 1002/20000 [00:00<00:01, 10011.77it/s]\u001b[A\n",
            "neural net for xor:  10%|▉         | 1961/20000 [00:00<00:01, 9880.96it/s] \u001b[A\n",
            "neural net for xor:  15%|█▍        | 2978/20000 [00:00<00:01, 9963.88it/s]\u001b[A\n",
            "neural net for xor:  20%|██        | 4022/20000 [00:00<00:01, 10100.30it/s]\u001b[A\n",
            "neural net for xor:  25%|██▌       | 5060/20000 [00:00<00:01, 10181.13it/s]\u001b[A\n",
            "neural net for xor:  30%|███       | 6059/20000 [00:00<00:01, 10122.08it/s]\u001b[A\n",
            "neural net for xor:  35%|███▌      | 7071/20000 [00:00<00:01, 10119.75it/s]\u001b[A\n",
            "neural net for xor:  40%|████      | 8001/20000 [00:00<00:01, 9702.41it/s] \u001b[A\n",
            "neural net for xor:  45%|████▌     | 9005/20000 [00:00<00:01, 9800.19it/s]\u001b[A\n",
            "neural net for xor:  50%|████▉     | 9948/20000 [00:01<00:01, 9511.94it/s]\u001b[A\n",
            "neural net for xor:  54%|█████▍    | 10875/20000 [00:01<00:00, 9370.67it/s]\u001b[A\n",
            "neural net for xor:  59%|█████▉    | 11847/20000 [00:01<00:00, 9471.56it/s]\u001b[A\n",
            "neural net for xor:  64%|██████▍   | 12826/20000 [00:01<00:00, 9564.38it/s]\u001b[A\n",
            "neural net for xor:  69%|██████▉   | 13833/20000 [00:01<00:00, 9709.04it/s]\u001b[A\n",
            "neural net for xor:  74%|███████▍  | 14806/20000 [00:01<00:00, 9715.17it/s]\u001b[A\n",
            "neural net for xor:  79%|███████▉  | 15775/20000 [00:01<00:00, 9448.37it/s]\u001b[A\n",
            "neural net for xor:  84%|████████▍ | 16815/20000 [00:01<00:00, 9713.22it/s]\u001b[A\n",
            "neural net for xor:  89%|████████▉ | 17802/20000 [00:01<00:00, 9756.68it/s]\u001b[A\n",
            "neural net for xor:  94%|█████████▍| 18784/20000 [00:01<00:00, 9774.83it/s]\u001b[A\n",
            "neural net for xor: 100%|██████████| 20000/20000 [00:02<00:00, 9723.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[7, 7, -3], [5, 5, -8]], [[11, -12, -5]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-ZP4Yhl5_gR"
      },
      "source": [
        "## **Example: Fizz Buzz**\n",
        "\n",
        "* Print the numbers 1 to 100, except that if the number is divisible by 3, print \"fizz\"; if the number is divisible by 5, print \"buzz\"; and if the number is divisible by 15, print \"fizzbuzz\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BehtQL4JJeQr"
      },
      "source": [
        "* Let's create a neural network with random initial weights. It will have 10 input neurons (since we're representing our inputs as 10-dimensional vectors) and 4 output neurons (since we're representing our targets as 4-dimensional vectors). \n",
        ">* We'll give it 25 hidden units, but we'll use a variable for that so it's easy to change:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvCRpbBIE09e",
        "outputId": "e8272ea6-7d71-4eaf-d67c-fa42ac8aedf2"
      },
      "source": [
        "def fizz_buzz_encode(x: int) -> Vector:\n",
        "    if x % 15 == 0:\n",
        "        return [0, 0, 0, 1]\n",
        "    elif x % 5 == 0:\n",
        "        return [0, 0, 1, 0]\n",
        "    elif x % 3 == 0:\n",
        "        return [0, 1, 0, 0]\n",
        "    else:\n",
        "        return [1, 0, 0, 0]\n",
        "\n",
        "assert fizz_buzz_encode(2) == [1, 0, 0, 0]\n",
        "assert fizz_buzz_encode(6) == [0, 1, 0, 0]\n",
        "assert fizz_buzz_encode(10) == [0, 0, 1, 0]\n",
        "assert fizz_buzz_encode(30) == [0, 0, 0, 1]\n",
        "\n",
        "def binary_encode(x: int) -> Vector:\n",
        "    binary: List[float] = []\n",
        "\n",
        "    for i in range(10):\n",
        "        binary.append(x % 2)\n",
        "        x = x // 2\n",
        "\n",
        "    return binary\n",
        "\n",
        "#                             1  2  4  8 16 32 64 128 256 512\n",
        "assert binary_encode(0)   == [0, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n",
        "assert binary_encode(1)   == [1, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n",
        "assert binary_encode(10)  == [0, 1, 0, 1, 0, 0, 0, 0,  0,  0]\n",
        "assert binary_encode(101) == [1, 0, 1, 0, 0, 1, 1, 0,  0,  0]\n",
        "assert binary_encode(999) == [1, 1, 1, 0, 0, 1, 1, 1,  1,  1]\n",
        "\n",
        "def argmax(xs: list) -> int:\n",
        "    \"\"\"Returns the index of the largest value\"\"\"\n",
        "    return max(range(len(xs)), key=lambda i: xs[i])\n",
        "\n",
        "assert argmax([0, -1]) == 0               # items[0] is largest\n",
        "assert argmax([-1, 0]) == 1               # items[1] is largest\n",
        "assert argmax([-1, 10, 5, 20, -3]) == 3   # items[3] is largest\n",
        "\n",
        "xs = [binary_encode(n) for n in range(101, 1024)]\n",
        "ys = [fizz_buzz_encode(n) for n in range(101, 1024)]\n",
        "    \n",
        "NUM_HIDDEN = 25\n",
        "    \n",
        "network = [\n",
        "    # hidden layer: 10 inputs -> NUM_HIDDEN outputs\n",
        "    [[random.random() for _ in range(10 + 1)] for _ in range(NUM_HIDDEN)],\n",
        "    \n",
        "    # output_layer: NUM_HIDDEN inputs -> 4 outputs\n",
        "    [[random.random() for _ in range(NUM_HIDDEN + 1)] for _ in range(4)]\n",
        "]\n",
        "    \n",
        "learning_rate = 1.0\n",
        "   \n",
        "with tqdm.trange(500) as t:\n",
        "    for epoch in t:\n",
        "        epoch_loss = 0.0\n",
        "    \n",
        "        for x, y in zip(xs, ys):\n",
        "            predicted = feed_forward(network, x)[-1]\n",
        "            epoch_loss += squared_distance(predicted, y)\n",
        "            gradients = sqerror_gradients(network, x, y)\n",
        "    \n",
        "            # Take a gradient step for each neuron in each layer\n",
        "            network = [[gradient_step(neuron, grad, -learning_rate)\n",
        "                        for neuron, grad in zip(layer, layer_grad)]\n",
        "                    for layer, layer_grad in zip(network, gradients)]\n",
        "    \n",
        "        t.set_description(f\"fizz buzz (loss: {epoch_loss:.2f})\")\n",
        "    \n",
        "num_correct = 0\n",
        "    \n",
        "for n in range(1, 101):\n",
        "    x = binary_encode(n)\n",
        "    predicted = argmax(feed_forward(network, x)[-1])\n",
        "    actual = argmax(fizz_buzz_encode(n))\n",
        "    labels = [str(n), \"fizz\", \"buzz\", \"fizzbuzz\"]\n",
        "    print(n, labels[predicted], labels[actual])\n",
        "    \n",
        "    if predicted == actual:\n",
        "        num_correct += 1\n",
        "    \n",
        "print(num_correct, \"/\", 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fizz buzz (loss: 29.68): 100%|██████████| 500/500 [02:56<00:00,  2.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 1 1\n",
            "2 2 2\n",
            "3 fizz fizz\n",
            "4 4 4\n",
            "5 buzz buzz\n",
            "6 fizz fizz\n",
            "7 7 7\n",
            "8 8 8\n",
            "9 fizz fizz\n",
            "10 buzz buzz\n",
            "11 11 11\n",
            "12 fizz fizz\n",
            "13 13 13\n",
            "14 14 14\n",
            "15 fizzbuzz fizzbuzz\n",
            "16 16 16\n",
            "17 17 17\n",
            "18 fizz fizz\n",
            "19 19 19\n",
            "20 20 buzz\n",
            "21 fizz fizz\n",
            "22 22 22\n",
            "23 23 23\n",
            "24 fizz fizz\n",
            "25 buzz buzz\n",
            "26 26 26\n",
            "27 fizz fizz\n",
            "28 28 28\n",
            "29 29 29\n",
            "30 fizzbuzz fizzbuzz\n",
            "31 31 31\n",
            "32 32 32\n",
            "33 fizz fizz\n",
            "34 34 34\n",
            "35 buzz buzz\n",
            "36 fizz fizz\n",
            "37 37 37\n",
            "38 38 38\n",
            "39 fizz fizz\n",
            "40 buzz buzz\n",
            "41 41 41\n",
            "42 fizz fizz\n",
            "43 43 43\n",
            "44 44 44\n",
            "45 fizzbuzz fizzbuzz\n",
            "46 46 46\n",
            "47 47 47\n",
            "48 fizz fizz\n",
            "49 49 49\n",
            "50 buzz buzz\n",
            "51 fizz fizz\n",
            "52 52 52\n",
            "53 53 53\n",
            "54 fizz fizz\n",
            "55 buzz buzz\n",
            "56 56 56\n",
            "57 fizz fizz\n",
            "58 58 58\n",
            "59 59 59\n",
            "60 fizzbuzz fizzbuzz\n",
            "61 61 61\n",
            "62 62 62\n",
            "63 fizz fizz\n",
            "64 64 64\n",
            "65 buzz buzz\n",
            "66 fizz fizz\n",
            "67 67 67\n",
            "68 68 68\n",
            "69 fizz fizz\n",
            "70 buzz buzz\n",
            "71 71 71\n",
            "72 fizz fizz\n",
            "73 73 73\n",
            "74 74 74\n",
            "75 fizzbuzz fizzbuzz\n",
            "76 76 76\n",
            "77 77 77\n",
            "78 fizz fizz\n",
            "79 79 79\n",
            "80 80 buzz\n",
            "81 fizz fizz\n",
            "82 82 82\n",
            "83 83 83\n",
            "84 fizz fizz\n",
            "85 fizz buzz\n",
            "86 86 86\n",
            "87 fizz fizz\n",
            "88 88 88\n",
            "89 89 89\n",
            "90 fizzbuzz fizzbuzz\n",
            "91 91 91\n",
            "92 92 92\n",
            "93 fizz fizz\n",
            "94 94 94\n",
            "95 fizz buzz\n",
            "96 fizz fizz\n",
            "97 97 97\n",
            "98 98 98\n",
            "99 fizz fizz\n",
            "100 buzz buzz\n",
            "96 / 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}