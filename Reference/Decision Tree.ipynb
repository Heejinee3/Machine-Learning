{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decision_trees.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mxzWG0juqnC"
      },
      "source": [
        "# **Chapter 17. Decision Trees**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qSNNS0z67w_"
      },
      "source": [
        "## What is a **Decision Tree**?\n",
        "\n",
        "* A decision tree uses a tree structure to represent a number of possible decision paths and an outcome for each path.\n",
        "* If you have ever played the game **Twenty Questions**, then you are familiar with decision trees. For example:\n",
        ">* \"I am thinking of an animal.\"\n",
        ">* \"Does it have more than five legs?\"\n",
        ">* \"No.\"\n",
        ">* \"Is it delicious?\"\n",
        ">* \"No.\"\n",
        ">* \"Does it appear on the back of the Australian five-cent coin?\"\n",
        ">* \"Yes.\"\n",
        ">* \"Is it an echidna(바늘두더지)?\"\n",
        ">* \"Yes, it is!\"\n",
        "\n",
        "* This corresponds to the path: \"Not more than 5 legs\" $\\rightarrow$ \"Not delicious\" $\\rightarrow$ \"On the 5-cent coin\" $\\rightarrow$ \"Echidna!\"\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1DdprSGfdNJugbWKuuVk3_JeyFJkvJ2AX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuFMTVbOSiMD"
      },
      "source": [
        "* Decision trees are very easy to understand and interpret, and the process by which they reach a prediction is completely transparent.\n",
        "\n",
        "* Decision trees can easily handle a mix of numeric (e.g., number of legs) and categorical (e.g., delicious/not delicious) attributes and can even classify data for which attributes are missing.\n",
        "\n",
        "* At the same time, finding an \"optimal\" decision tree for a set of training data is computationally a very hard problem. We will get around this by trying to build a good enough tree rather than an optimal one, although for large datasets this can still be a lot of work.\n",
        "\n",
        "* More important, it is very easy (and very bad) to build decision trees that are overfitted to the training data, and that don't generalize well to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9Pir3nqy2G8"
      },
      "source": [
        "* We will work through the **ID3** algorithm for learning a\n",
        "decision tree from a set of labeled data.\n",
        ">* CART, ID3, and C4.5 algorithms are the representative algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npHSkn3JzaVR"
      },
      "source": [
        "## **Entropy**\n",
        "\n",
        "* In order to build a decision tree, we will need to decide _what questions to ask_ and _in what order_. \n",
        ">* At each stage of the tree there are some possibilities we've eliminated and some that we haven't. After learning that an animal doesn't have more than five legs, we've eliminated the possibility that it's a grasshopper. We haven't eliminated the possibility that it's a duck. Each possible question partitions the remaining possibilities according to its answer.\n",
        "\n",
        "* Ideally, we'd like to choose questions whose answers give a lot of information about what our tree should predict. \n",
        ">* If there's a single yes/no question for which \"yes\" answers always correspond to `True` outputs and \"no\" answers to `False` outputs (or vice versa), this would be an awesome question to pick. Conversely, a yes/no question\n",
        "for which neither answer gives you much new information about what the prediction should be is probably not a good choice.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1_PK8z-NAvzPUqrzBgQnN3PlCQIliHgzd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_DzqnSJ1Tq0"
      },
      "source": [
        "* Imagine that we have a set $S$ of data, each member of which is labeled as belonging to one of a finite number of classes $C_1, \\ldots, C_n$. If all the data points belong to a single class, then there is no real uncertainty, which means we'd like there to be *low* entropy. If the data points are evenly spread across the classes, there is a lot of uncertainty and we'd like there to be *high* entropy.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1zFP-AirUZTHipNp47kT8HaFobzJ3vSfe)\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1BaMjxjoH5O8eV9bhJrAFffepx5E56G6Z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wQIvdbpnO8n"
      },
      "source": [
        "* In math terms, if $p_i$ is the proportion of data labeled as class $c_i$, we define the entropy as: $$H(S) = -p_1\\log_2 p_1-\\cdots-p_n\\log_2 p_n $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "6Xo7xpFPNJcS",
        "outputId": "c20db8e5-815b-42fd-b917-526065b17a85"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "\n",
        "x = np.linspace(0,1)\n",
        "y = entropy([x, 1-x], base=2)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.xlabel('p')\n",
        "plt.ylabel('Entropy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOftlMXIooZE"
      },
      "source": [
        "from typing import List\n",
        "import math\n",
        "\n",
        "def entropy(class_probabilities: List[float]) -> float:\n",
        "    \"\"\"Given a list of class probabilities, compute the entropy\"\"\"\n",
        "    return sum(-p * math.log(p, 2)\n",
        "               for p in class_probabilities\n",
        "               if p > 0)                     # ignore zero probabilities\n",
        "\n",
        "assert entropy([1.0]) == 0\n",
        "assert entropy([0.5, 0.5]) == 1\n",
        "assert 0.81 < entropy([0.25, 0.75]) < 0.82\n",
        "\n",
        "from typing import Any\n",
        "from collections import Counter\n",
        "\n",
        "def class_probabilities(labels: List[Any]) -> List[float]:\n",
        "    total_count = len(labels)\n",
        "    return [count / total_count\n",
        "            for count in Counter(labels).values()]\n",
        "\n",
        "def data_entropy(labels: List[Any]) -> float:\n",
        "    return entropy(class_probabilities(labels))\n",
        "\n",
        "assert data_entropy(['a']) == 0\n",
        "assert data_entropy([True, False]) == 1\n",
        "assert data_entropy([3, 4, 4, 4]) == entropy([0.25, 0.75])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF3p4un5oo2T"
      },
      "source": [
        "## **The Entropy of a Partition**\n",
        "\n",
        "* We'd like some notion of the entropy that results from partitioning a set of data in a certain way. We want a partition to have low entropy if it splits the data into subsets that themselves have low entropy (i.e., are highly certain), and high entropy if it contains subsets that (are large and) have high entropy (i.e., are highly uncertain).\n",
        "\n",
        "* Mathematically, if we partition our data $S$ into subsets $S_1, \\ldots, S_m$ containing proportions $q_1, \\ldots, q_m$ of the data, then we compute the entropy of the partition as a weighted sum:\n",
        "$$H = q_1H(S_1) +\\cdots+q_mH(S_m)$$\n",
        "\n",
        "* The difference in the entropy before and after the split is called the **information gain**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cirSVnHqBSn"
      },
      "source": [
        "def partition_entropy(subsets: List[List[Any]]) -> float:\n",
        "    \"\"\"Returns the entropy from this partition of data into subsets\"\"\"\n",
        "    total_count = sum(len(subset) for subset in subsets)\n",
        "\n",
        "    return sum(data_entropy(subset) * len(subset) / total_count\n",
        "               for subset in subsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv54ExG8p7Dp"
      },
      "source": [
        "## **Creating a Decisin Tree**\n",
        "\n",
        "* Our tree will consist of **decision nodes** (which ask a question and direct us differently depending on the answer) and **leaf nodes** (which give us a prediction). We will build it using the relatively simple **ID3** algorithm, which operates in the following manner. Let's say we're given some labeled data, and a list of attributes to consider branching on:\n",
        ">* If the data all have the same label, create a leaf node that predicts that label and then stop.\n",
        ">* If the list of attributes is empty (i.e., there are no more possible questions to ask), create a leaf node that predicts the most common label and then stop.\n",
        ">* Otherwise, try partitioning the data by each of the attributes.\n",
        ">* Choose the partition with the lowest partition entropy.\n",
        ">* Add a decision node based on the chosen attribute.\n",
        ">* Recur on each partitioned subset using the remaining attributes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXV7C-FbtYrg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b8afd45-f048-46a3-c48e-86d6f6c998e8"
      },
      "source": [
        "from typing import NamedTuple, Optional\n",
        "\n",
        "class Candidate(NamedTuple):\n",
        "    level: str\n",
        "    lang: str\n",
        "    tweets: bool\n",
        "    phd: bool\n",
        "    did_well: Optional[bool] = None  # allow unlabeled data\n",
        "\n",
        "                  #  level     lang     tweets  phd  did_well\n",
        "inputs = [Candidate('Senior', 'Java',   False, False, False),\n",
        "          Candidate('Senior', 'Java',   False, True,  False),\n",
        "          Candidate('Mid',    'Python', False, False, True),\n",
        "          Candidate('Junior', 'Python', False, False, True),\n",
        "          Candidate('Junior', 'R',      True,  False, True),\n",
        "          Candidate('Junior', 'R',      True,  True,  False),\n",
        "          Candidate('Mid',    'R',      True,  True,  True),\n",
        "          Candidate('Senior', 'Python', False, False, False),\n",
        "          Candidate('Senior', 'R',      True,  False, True),\n",
        "          Candidate('Junior', 'Python', True,  False, True),\n",
        "          Candidate('Senior', 'Python', True,  True,  True),\n",
        "          Candidate('Mid',    'Python', False, True,  True),\n",
        "          Candidate('Mid',    'Java',   True,  False, True),\n",
        "          Candidate('Junior', 'Python', False, True,  False)\n",
        "         ]\n",
        "\n",
        "from typing import Dict, TypeVar\n",
        "from collections import defaultdict\n",
        "\n",
        "T = TypeVar('T')  # generic type for inputs\n",
        "\n",
        "def partition_by(inputs: List[T], attribute: str) -> Dict[Any, List[T]]:\n",
        "    \"\"\"Partition the inputs into lists based on the specified attribute.\"\"\"\n",
        "    partitions: Dict[Any, List[T]] = defaultdict(list)\n",
        "    for input in inputs:\n",
        "        key = getattr(input, attribute)  # value of the specified attribute\n",
        "        partitions[key].append(input)    # add input to the correct partition\n",
        "    return partitions\n",
        "\n",
        "def partition_entropy_by(inputs: List[Any],\n",
        "                         attribute: str,\n",
        "                         label_attribute: str) -> float:\n",
        "    \"\"\"Compute the entropy corresponding to the given partition\"\"\"\n",
        "    # partitions consist of our inputs\n",
        "    partitions = partition_by(inputs, attribute)\n",
        "\n",
        "    # but partition_entropy needs just the class labels\n",
        "    labels = [[getattr(input, label_attribute) for input in partition]\n",
        "              for partition in partitions.values()]\n",
        "\n",
        "    return partition_entropy(labels)\n",
        "\n",
        "for key in ['level','lang','tweets','phd']:\n",
        "    print(key, partition_entropy_by(inputs, key, 'did_well'))\n",
        "\n",
        "assert 0.69 < partition_entropy_by(inputs, 'level', 'did_well')  < 0.70\n",
        "assert 0.86 < partition_entropy_by(inputs, 'lang', 'did_well')   < 0.87\n",
        "assert 0.78 < partition_entropy_by(inputs, 'tweets', 'did_well') < 0.79\n",
        "assert 0.89 < partition_entropy_by(inputs, 'phd', 'did_well')    < 0.90\n",
        "\n",
        "senior_inputs = [input for input in inputs if input.level == 'Senior']\n",
        "\n",
        "assert 0.4 == partition_entropy_by(senior_inputs, 'lang', 'did_well')\n",
        "assert 0.0 == partition_entropy_by(senior_inputs, 'tweets', 'did_well')\n",
        "assert 0.95 < partition_entropy_by(senior_inputs, 'phd', 'did_well') < 0.96"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "level 0.6935361388961919\n",
            "lang 0.8601317128547441\n",
            "tweets 0.7884504573082896\n",
            "phd 0.8921589282623617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTkaKZ2YtY67"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1B_Acd0OoJfG4NacP5U_WN8CHMqFz9pH6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpzuRf4gtkrd"
      },
      "source": [
        "## **Putting It All Together**\n",
        "\n",
        "* We would like to implement it more generally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNLyr_5HgIMG"
      },
      "source": [
        "from typing import NamedTuple, Union, Any\n",
        "\n",
        "class Leaf(NamedTuple):\n",
        "    value: Any\n",
        "\n",
        "class Split(NamedTuple):\n",
        "    attribute: str\n",
        "    subtrees: dict\n",
        "    default_value: Any = None\n",
        "\n",
        "DecisionTree = Union[Leaf, Split]\n",
        "\n",
        "hiring_tree = Split('level', {   # First, consider \"level\".\n",
        "    'Junior': Split('phd', {     # if level is \"Junior\", next look at \"phd\"\n",
        "        False: Leaf(True),       #   if \"phd\" is False, predict True\n",
        "        True: Leaf(False)        #   if \"phd\" is True, predict False\n",
        "    }),\n",
        "    'Mid': Leaf(True),           # if level is \"Mid\", just predict True\n",
        "    'Senior': Split('tweets', {  # if level is \"Senior\", look at \"tweets\"\n",
        "        False: Leaf(False),      #   if \"tweets\" is False, predict False\n",
        "        True: Leaf(True)         #   if \"tweets\" is True, predict True\n",
        "    })\n",
        "})\n",
        "\n",
        "def classify(tree: DecisionTree, input: Any) -> Any:\n",
        "    \"\"\"classify the input using the given decision tree\"\"\"\n",
        "\n",
        "    # If this is a leaf node, return its value\n",
        "    if isinstance(tree, Leaf):\n",
        "        return tree.value\n",
        "\n",
        "    # Otherwise this tree consists of an attribute to split on\n",
        "    # and a dictionary whose keys are values of that attribute\n",
        "    # and whose values of are subtrees to consider next\n",
        "    subtree_key = getattr(input, tree.attribute)\n",
        "\n",
        "    if subtree_key not in tree.subtrees:   # If no subtree for key,\n",
        "        return tree.default_value          # return the default value.\n",
        "\n",
        "    subtree = tree.subtrees[subtree_key]   # Choose the appropriate subtree\n",
        "    return classify(subtree, input)        # and use it to classify the input.\n",
        "\n",
        "def build_tree_id3(inputs: List[Any],\n",
        "                   split_attributes: List[str],\n",
        "                   target_attribute: str) -> DecisionTree:\n",
        "    # Count target labels\n",
        "    label_counts = Counter(getattr(input, target_attribute)\n",
        "                           for input in inputs)\n",
        "    most_common_label = label_counts.most_common(1)[0][0]\n",
        "\n",
        "    # If there's a unique label, predict it\n",
        "    if len(label_counts) == 1:\n",
        "        return Leaf(most_common_label)\n",
        "\n",
        "    # If no split attributes left, return the majority label\n",
        "    if not split_attributes:\n",
        "        return Leaf(most_common_label)\n",
        "\n",
        "    # Otherwise split by the best attribute\n",
        "\n",
        "    def split_entropy(attribute: str) -> float:\n",
        "        \"\"\"Helper function for finding the best attribute\"\"\"\n",
        "        return partition_entropy_by(inputs, attribute, target_attribute)\n",
        "\n",
        "    best_attribute = min(split_attributes, key=split_entropy)\n",
        "\n",
        "    partitions = partition_by(inputs, best_attribute)\n",
        "    new_attributes = [a for a in split_attributes if a != best_attribute]\n",
        "\n",
        "    # recursively build the subtrees\n",
        "    subtrees = {attribute_value : build_tree_id3(subset,\n",
        "                                                 new_attributes,\n",
        "                                                 target_attribute)\n",
        "                for attribute_value, subset in partitions.items()}\n",
        "\n",
        "    return Split(best_attribute, subtrees, default_value=most_common_label)\n",
        "\n",
        "tree = build_tree_id3(inputs,\n",
        "                      ['level', 'lang', 'tweets', 'phd'],\n",
        "                      'did_well')\n",
        "\n",
        "# Should predict True\n",
        "assert classify(tree, Candidate(\"Junior\", \"Java\", True, False))\n",
        "\n",
        "# Should predict False\n",
        "assert not classify(tree, Candidate(\"Junior\", \"Java\", True, True))\n",
        "\n",
        "# Should predict True\n",
        "assert classify(tree, Candidate(\"Intern\", \"Java\", True, True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4erkJLUd5B7"
      },
      "source": [
        "## **Using scikit-learn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "oDMGsSsrJY6P",
        "outputId": "adf872fb-32a9-40d0-9797-c7cf1ace23bf"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, 2:] # petal length and width\n",
        "y = iris.target\n",
        "\n",
        "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
        "tree_clf.fit(X, y)\n",
        "\n",
        "from graphviz import Source\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "export_graphviz(\n",
        "        tree_clf,\n",
        "        out_file=\"iris_tree.dot\",\n",
        "        feature_names=iris.feature_names[2:],\n",
        "        class_names=iris.target_names,\n",
        "        rounded=True,\n",
        "        filled=True\n",
        "    )\n",
        "\n",
        "Source.from_file(\"iris_tree.dot\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.files.Source at 0x7f1ddf254150>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"368pt\" height=\"314pt\"\n viewBox=\"0.00 0.00 368.00 314.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 310)\">\n<title>Tree</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-310 364,-310 364,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#ffffff\" stroke=\"#000000\" d=\"M219.5,-306C219.5,-306 67.5,-306 67.5,-306 61.5,-306 55.5,-300 55.5,-294 55.5,-294 55.5,-235 55.5,-235 55.5,-229 61.5,-223 67.5,-223 67.5,-223 219.5,-223 219.5,-223 225.5,-223 231.5,-229 231.5,-235 231.5,-235 231.5,-294 231.5,-294 231.5,-300 225.5,-306 219.5,-306\"/>\n<text text-anchor=\"middle\" x=\"143.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">petal length (cm) &lt;= 2.45</text>\n<text text-anchor=\"middle\" x=\"143.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.667</text>\n<text text-anchor=\"middle\" x=\"143.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 150</text>\n<text text-anchor=\"middle\" x=\"143.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [50, 50, 50]</text>\n<text text-anchor=\"middle\" x=\"143.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = setosa</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"#000000\" d=\"M111,-179.5C111,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 111,-111.5 111,-111.5 117,-111.5 123,-117.5 123,-123.5 123,-123.5 123,-167.5 123,-167.5 123,-173.5 117,-179.5 111,-179.5\"/>\n<text text-anchor=\"middle\" x=\"61.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n<text text-anchor=\"middle\" x=\"61.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 50</text>\n<text text-anchor=\"middle\" x=\"61.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [50, 0, 0]</text>\n<text text-anchor=\"middle\" x=\"61.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M114.8204,-222.8796C107.0917,-211.6636 98.7191,-199.5131 90.9492,-188.2372\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"93.7018,-186.0634 85.1456,-179.8149 87.9377,-190.0353 93.7018,-186.0634\"/>\n<text text-anchor=\"middle\" x=\"80.6196\" y=\"-200.7018\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#ffffff\" stroke=\"#000000\" d=\"M299.5,-187C299.5,-187 153.5,-187 153.5,-187 147.5,-187 141.5,-181 141.5,-175 141.5,-175 141.5,-116 141.5,-116 141.5,-110 147.5,-104 153.5,-104 153.5,-104 299.5,-104 299.5,-104 305.5,-104 311.5,-110 311.5,-116 311.5,-116 311.5,-175 311.5,-175 311.5,-181 305.5,-187 299.5,-187\"/>\n<text text-anchor=\"middle\" x=\"226.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">petal width (cm) &lt;= 1.75</text>\n<text text-anchor=\"middle\" x=\"226.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.5</text>\n<text text-anchor=\"middle\" x=\"226.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 100</text>\n<text text-anchor=\"middle\" x=\"226.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 50, 50]</text>\n<text text-anchor=\"middle\" x=\"226.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M172.5294,-222.8796C178.6855,-214.0534 185.2451,-204.6485 191.5936,-195.5466\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"194.4967,-197.5024 197.3467,-187.2981 188.7552,-193.4978 194.4967,-197.5024\"/>\n<text text-anchor=\"middle\" x=\"201.7331\" y=\"-208.2103\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#4de88e\" stroke=\"#000000\" d=\"M206.5,-68C206.5,-68 102.5,-68 102.5,-68 96.5,-68 90.5,-62 90.5,-56 90.5,-56 90.5,-12 90.5,-12 90.5,-6 96.5,0 102.5,0 102.5,0 206.5,0 206.5,0 212.5,0 218.5,-6 218.5,-12 218.5,-12 218.5,-56 218.5,-56 218.5,-62 212.5,-68 206.5,-68\"/>\n<text text-anchor=\"middle\" x=\"154.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.168</text>\n<text text-anchor=\"middle\" x=\"154.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 54</text>\n<text text-anchor=\"middle\" x=\"154.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 49, 5]</text>\n<text text-anchor=\"middle\" x=\"154.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M199.6899,-103.9815C193.9331,-95.0666 187.8404,-85.6313 182.0559,-76.6734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"184.9904,-74.7658 176.6254,-68.2637 179.1099,-78.5631 184.9904,-74.7658\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#843de6\" stroke=\"#000000\" d=\"M348,-68C348,-68 249,-68 249,-68 243,-68 237,-62 237,-56 237,-56 237,-12 237,-12 237,-6 243,0 249,0 249,0 348,0 348,0 354,0 360,-6 360,-12 360,-12 360,-56 360,-56 360,-62 354,-68 348,-68\"/>\n<text text-anchor=\"middle\" x=\"298.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.043</text>\n<text text-anchor=\"middle\" x=\"298.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 46</text>\n<text text-anchor=\"middle\" x=\"298.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1, 45]</text>\n<text text-anchor=\"middle\" x=\"298.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>2&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M253.3101,-103.9815C259.0669,-95.0666 265.1596,-85.6313 270.9441,-76.6734\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"273.8901,-78.5631 276.3746,-68.2637 268.0096,-74.7658 273.8901,-78.5631\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}