{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "naive_bayes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mxzWG0juqnC"
      },
      "source": [
        "# **Chapter 13. Naive Bayes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLwAnW2YKAKP"
      },
      "source": [
        "## **Bayes' Theorem**\n",
        "\n",
        "* In probability theory and statistics, **Bayes' theorem** describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n",
        "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
        "Alternative form:\n",
        "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|\\neg A)P(\\neg A)} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO_H5_cVtq1g"
      },
      "source": [
        "## **A Really Dumb Spam Filter**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9CFUPpJtrcM"
      },
      "source": [
        "* Let $S$ be the event \"the message is spam\" and $B$ be the event \"the message contains the word bitcoin.\" Bayes's theorem tells us that the probability that the message is spam conditional on containing the word bitcoin is:\n",
        "$$P (S|B) = \\frac{P (B | S) P (S)}{P (B | S) P (S) + P (B | \\neg S) P (\\neg S)}$$\n",
        ">* The numerator is the probability that a message is spam _and_ contains _bitcoin_, while the denominator is just the probability that a message contains _bitcoin_. Hence, you can think of this calculation as simply representing the proportion of _bitcoin_ messages that are spam.\n",
        "\n",
        "* If we have a large collection of messages we know are spam, and a large collection of messages we know are not spam, then we can easily estimate $P(B|S)$ and $P(B|\\neg S)$. If we further assume that any message is equally likely to be spam or not spam (so that $P(S) = P(\\neg S) = 0.5$), then:\n",
        "$$P (S | B) = \\frac{P (B | S)}{P (B | S) + P (B | \\neg S)}$$\n",
        "\n",
        "* For example, if 50% of spam messages have the word bitcoin, but only 1% of nonspam messages do, then the probability that any given bitcoin-containing email is spam is: $\\frac{0.5}{0.5+0.01} =98\\%$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htqMrEre5PxI"
      },
      "source": [
        "## **A More Sophisticated Spam Filter**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex7m_Oj-_34Y"
      },
      "source": [
        "* Suppose that we have a vocabulary of many words, $w_1, \\ldots, w_n$. To move this into the realm of probability theory, we'll write $X_i$ for the event \"a message contains the word $w_i$.\" Also imagine that we've come up with an estimate $P(X_i|S)$ for the probability that a spam message contains the $i$-th word, and a similar estimate $P(X_i|\\neg S)$ for the probability that a nonspam message contains the $i$-th word.\n",
        "\n",
        "* The key to Naive Bayes is making the assumption that the presences (or absences) of each word are **independent of one another**, conditional on a message being spam or not. In math terms, this means that:\n",
        "$$P(X_1=x_1,\\ldots,X_n=x_n | S) =P(X_1=x_1 | S) \\times \\cdots \\times P(X_n=x_n | S)$$\n",
        "\n",
        "* This is an extreme assumption. (There’s a reason the technique has **naive** in its name.) Despite the unrealisticness of this assumption, this model often performs well and has historically been used in actual spam filters.\n",
        "\n",
        "* The same Bayes's theorem reasoning we used for our \"bitcoin-only\" spam filter tells us that we can calculate the probability a message is spam using the equation:\n",
        "$$ P (S | X = x) = \\frac{P(X = x | S)}{P (X = x | S) + P (X = x | \\neg S)}$$\n",
        "\n",
        "* The Naive Bayes assumption allows us to compute each of the probabilities on the right simply by multiplying together the individual probability estimates for each vocabulary word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oj0O4fKEZDb"
      },
      "source": [
        "* In practice, you usually want to avoid multiplying lots of probabilities together, to prevent a problem called *underflow*, in which computers don't deal well with floating-point numbers that are too close to 0. Recalling from algebra that $\\log (ab) = \\log a + \\log b$ and that $\\exp(\\log x) = x$, we usually compute $p_1 \\times \\cdots \\times p_n$ as the equivalent (but floating-point-friendlier): $\\exp (\\log p_1 + \\cdots + \\log p_n)$.\n",
        "\n",
        "* The only challenge left is coming up with estimates for $P (X_i | S)$ and $P(X_i | \\neg S)$, the probabilities that a spam message (or nonspam message) contains the word $w_i$. If we have a fair number of training messages labeled as spam and not spam, an obvious first try is to estimate $P(X_i | S)$ simply as the fraction of spam messages containing the word $w_i$.\n",
        "\n",
        "* Suppose that in our training set the vocabulary word *data* only occurs in nonspam messages. Then we'd estimate $P( \\text{data} | S) = 0$. The result is that our Naive Bayes classifier would always assign spam probability $0$ to any message containing the word *data*, even a message like \"data on free bitcoin and authentic rolex watches.\" To avoid this problem, we usually use some kind of smoothing.\n",
        ">* Refer to **Laplace smoothing** at this [link](https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece).\n",
        "\n",
        "* In particular, we'll choose a pseudocount—$k$—and estimate the probability of seeing the $i$-th word in a spam message as:\n",
        "$$P (X_i | S) = \\frac{k + \\text{number of spams containing}~w_i}{2k + \\text{number of spams}}$$\n",
        "\n",
        "* For example, if *data* does **not** occur in $98$ spam messages, and if $k$ is 1, we estimate $P(\\text{data}|S)$ as $\\frac{1}{100} = 0.01$, which allows our classifier to still assign some nonzero spam probability to messages that contain the word *data*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDMkxtoVEZ_s"
      },
      "source": [
        "## **Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAo7rs0quXTS"
      },
      "source": [
        "from typing import Set\n",
        "import re\n",
        "\n",
        "def tokenize(text: str) -> Set[str]:\n",
        "    text = text.lower()                         # Convert to lowercase,\n",
        "    all_words = re.findall(\"[a-z0-9']+\", text)  # extract the words, and\n",
        "    return set(all_words)                       # remove duplicates.\n",
        "\n",
        "assert tokenize(\"Data Science is science\") == {\"data\", \"science\", \"is\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiDrkQTouhw9"
      },
      "source": [
        "from typing import NamedTuple\n",
        "\n",
        "class Message(NamedTuple):\n",
        "    text: str\n",
        "    is_spam: bool"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikGELKnH1ppa"
      },
      "source": [
        "* Ultimately, we will want to predict $P(S | token)$.\n",
        "* _probabilities(): To apply Bayes's theorem we need to know $P(token | S)$ and $P(token | \\neg S)$ for each token in the vocabulary.\n",
        "* predict(): we will sum up the log probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Evjld1euk5t"
      },
      "source": [
        "from typing import List, Tuple, Dict, Iterable\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self, k: float = 0.5) -> None:\n",
        "        self.k = k  # smoothing factor\n",
        "\n",
        "        self.tokens: Set[str] = set()\n",
        "        self.token_spam_counts: Dict[str, int] = defaultdict(int)\n",
        "        self.token_ham_counts: Dict[str, int] = defaultdict(int)\n",
        "        self.spam_messages = self.ham_messages = 0\n",
        "\n",
        "    def train(self, messages: Iterable[Message]) -> None:\n",
        "        for message in messages:\n",
        "            # Increment message counts\n",
        "            if message.is_spam:\n",
        "                self.spam_messages += 1\n",
        "            else:\n",
        "                self.ham_messages += 1\n",
        "\n",
        "            # Increment word counts\n",
        "            for token in tokenize(message.text):\n",
        "                self.tokens.add(token)\n",
        "                if message.is_spam:\n",
        "                    self.token_spam_counts[token] += 1\n",
        "                else:\n",
        "                    self.token_ham_counts[token] += 1\n",
        "\n",
        "    def _probabilities(self, token: str) -> Tuple[float, float]:\n",
        "        \"\"\"returns P(token | spam) and P(token | not spam)\"\"\"\n",
        "        spam = self.token_spam_counts[token]\n",
        "        ham = self.token_ham_counts[token]\n",
        "\n",
        "        p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n",
        "        p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k)\n",
        "\n",
        "        return p_token_spam, p_token_ham\n",
        "\n",
        "    def predict(self, text: str) -> float:\n",
        "        text_tokens = tokenize(text)\n",
        "        log_prob_if_spam = log_prob_if_ham = 0.0\n",
        "\n",
        "        # Iterate through each word in our vocabulary.\n",
        "        for token in self.tokens:\n",
        "            prob_if_spam, prob_if_ham = self._probabilities(token)\n",
        "\n",
        "            # If *token* appears in the message,\n",
        "            # add the log probability of seeing it;\n",
        "            if token in text_tokens:\n",
        "                log_prob_if_spam += math.log(prob_if_spam)\n",
        "                log_prob_if_ham += math.log(prob_if_ham)\n",
        "\n",
        "            # otherwise add the log probability of _not_ seeing it\n",
        "            # which is log(1 - probability of seeing it)\n",
        "            else:\n",
        "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
        "                log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
        "\n",
        "        prob_if_spam = math.exp(log_prob_if_spam)\n",
        "        prob_if_ham = math.exp(log_prob_if_ham)\n",
        "        return prob_if_spam / (prob_if_spam + prob_if_ham)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnjed5ws2zEl"
      },
      "source": [
        "## **Testing Our Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml0iy22Zux8K"
      },
      "source": [
        "messages = [Message(\"spam rules\", is_spam=True),\n",
        "            Message(\"ham rules\", is_spam=False),\n",
        "            Message(\"hello ham\", is_spam=False)]\n",
        "\n",
        "model = NaiveBayesClassifier(k=0.5)\n",
        "model.train(messages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVVQ-RpVu3a1"
      },
      "source": [
        "assert model.tokens == {\"spam\", \"ham\", \"rules\", \"hello\"}\n",
        "assert model.spam_messages == 1\n",
        "assert model.ham_messages == 2\n",
        "assert model.token_spam_counts == {\"spam\": 1, \"rules\": 1}\n",
        "assert model.token_ham_counts == {\"ham\": 2, \"rules\": 1, \"hello\": 1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO95I-oJu5yM"
      },
      "source": [
        "text = \"hello spam\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmQSvStCP34_"
      },
      "source": [
        "* $P(\\text{\"spam\"} | S) \\times P(\\neg \\text{\"ham\"} | S) \\times P(\\neg \\text{\"rules\"} | S) \\times P(\\text{\"hello\"} | S)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBPGWESxPzoW"
      },
      "source": [
        "probs_if_spam = [\n",
        "    (1 + 0.5) / (1 + 2 * 0.5),      # \"spam\"  (present)\n",
        "    1 - (0 + 0.5) / (1 + 2 * 0.5),  # \"ham\"   (not present)\n",
        "    1 - (1 + 0.5) / (1 + 2 * 0.5),  # \"rules\" (not present)\n",
        "    (0 + 0.5) / (1 + 2 * 0.5)       # \"hello\" (present)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jee21r3cPsER"
      },
      "source": [
        "* $P(\\text{\"spam\"} | \\neg S) \\times P(\\neg \\text{\"ham\"} | \\neg  S) \\times P(\\neg \\text{\"rules\"} | \\neg  S) \\times P(\\text{\"hello\"} | \\neg  S)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbeN0DCHPtHh"
      },
      "source": [
        "probs_if_ham = [\n",
        "    (0 + 0.5) / (2 + 2 * 0.5),      # \"spam\"  (present)\n",
        "    1 - (2 + 0.5) / (2 + 2 * 0.5),  # \"ham\"   (not present)\n",
        "    1 - (1 + 0.5) / (2 + 2 * 0.5),  # \"rules\" (not present)\n",
        "    (1 + 0.5) / (2 + 2 * 0.5),      # \"hello\" (present)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN2qpaY-Pwjp"
      },
      "source": [
        "p_if_spam = math.exp(sum(math.log(p) for p in probs_if_spam))\n",
        "p_if_ham = math.exp(sum(math.log(p) for p in probs_if_ham))\n",
        "\n",
        "# Should be about 0.83\n",
        "assert model.predict(text) == p_if_spam / (p_if_spam + p_if_ham)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4qgqh3124q6"
      },
      "source": [
        "## **Using Our Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47_AaCjc4Xya"
      },
      "source": [
        "* A popular (if somewhat old) dataset is the SpamAssassin public corpus. We'll look at the files prefixed with *20021010*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0daE5vk3yLd"
      },
      "source": [
        "from io import BytesIO # So we can treat bytes as a file.\n",
        "import requests # To download the files, which\n",
        "import tarfile # are in .tar.bz format.\n",
        "\n",
        "BASE_URL = \"https://spamassassin.apache.org/old/publiccorpus\"\n",
        "FILES = [\"20021010_easy_ham.tar.bz2\",\n",
        "         \"20021010_hard_ham.tar.bz2\",\n",
        "         \"20021010_spam.tar.bz2\"]\n",
        "\n",
        "# This is where the data will end up,\n",
        "# in /spam, /easy_ham, and /hard_ham subdirectories.\n",
        "# Change this to where you want the data.\n",
        "OUTPUT_DIR = 'spam_data'\n",
        "\n",
        "for filename in FILES:\n",
        "    # Use requests to get the file contents at each URL.\n",
        "    content = requests.get(f\"{BASE_URL}/{filename}\").content\n",
        "    # Wrap the in-memory bytes so we can use them as a \"file.\"\n",
        "    fin = BytesIO(content)\n",
        "    # And extract all the files to the specified output dir.\n",
        "    with tarfile.open(fileobj=fin, mode='r:bz2') as tf:\n",
        "        tf.extractall(OUTPUT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFZVELeutrmX"
      },
      "source": [
        "import glob, re\n",
        "    \n",
        "# modify the path to wherever you've put the files\n",
        "path = 'spam_data/*/*'\n",
        "    \n",
        "data: List[Message] = []\n",
        "    \n",
        "# glob.glob returns every filename that matches the wildcarded path\n",
        "for filename in glob.glob(path):\n",
        "    is_spam = \"ham\" not in filename\n",
        "    \n",
        "    # There are some garbage characters in the emails, the errors='ignore'\n",
        "    # skips them instead of raising an exception.\n",
        "    with open(filename, errors='ignore') as email_file:\n",
        "        for line in email_file:\n",
        "            if line.startswith(\"Subject:\"):\n",
        "                subject = line.lstrip(\"Subject: \")\n",
        "                data.append(Message(subject, is_spam))\n",
        "                break  # done with this file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW-6z4qJv11p"
      },
      "source": [
        "import requests\n",
        "url = 'https://raw.githubusercontent.com/joelgrus/data-science-from-scratch/master/scratch/machine_learning.py'\n",
        "r = requests.get(url)\n",
        "\n",
        "with open('machine_learning.py', 'w') as f:\n",
        "    f.write(r.text)\n",
        "\n",
        "import random\n",
        "from machine_learning import split_data\n",
        "    \n",
        "random.seed(0)      # just so you get the same answers as me\n",
        "train_messages, test_messages = split_data(data, 0.75)\n",
        "    \n",
        "model = NaiveBayesClassifier()\n",
        "model.train(train_messages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkAlPUrdvsux",
        "outputId": "9646bc48-1759-47e0-f66a-4d11a6b3ee92"
      },
      "source": [
        "from collections import Counter\n",
        "    \n",
        "predictions = [(message, model.predict(message.text))\n",
        "               for message in test_messages]\n",
        "    \n",
        "# Assume that spam_probability > 0.5 corresponds to spam prediction\n",
        "# and count the combinations of (actual is_spam, predicted is_spam)\n",
        "confusion_matrix = Counter((message.is_spam, spam_probability > 0.5)\n",
        "                            for message, spam_probability in predictions)\n",
        "    \n",
        "print(confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({(False, False): 674, (True, True): 89, (True, False): 37, (False, True): 25})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsbE9z7I6HOI"
      },
      "source": [
        "* This gives 89 true positives (spam classified as \"spam\"), 25 false positives (ham classified as \"spam\"), 674 true negatives (ham classified as \"ham\"), and 37 false negatives (spam classified as \"ham\"). This means our precision is $\\frac{89}{89 + 25} = 78\\%$, and our recall is $\\frac{89}{89 + 37} = 71\\%$, which are not bad numbers for such a simple model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFW4PKnnvfE9",
        "outputId": "a743df1b-e9b2-41c3-905a-741ae1524d89"
      },
      "source": [
        "def p_spam_given_token(token: str, model: NaiveBayesClassifier) -> float:\n",
        "    # We probably shouldn't call private methods, but it's for a good cause.\n",
        "    prob_if_spam, prob_if_ham = model._probabilities(token)\n",
        "    \n",
        "    return prob_if_spam / (prob_if_spam + prob_if_ham)\n",
        "    \n",
        "words = sorted(model.tokens, key=lambda t: p_spam_given_token(t, model))\n",
        "    \n",
        "print(\"spammiest_words\", words[-10:])\n",
        "print(\"hammiest_words\", words[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spammiest_words ['clearance', 'sale', '95', 'per', 'zzzz', 'only', 'money', 'systemworks', 'rates', 'adv']\n",
            "hammiest_words ['spambayes', 'users', 'zzzzteana', 'razor', 'sadev', 'ouch', 'apt', 'spamassassin', 'spam', 'bliss']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}